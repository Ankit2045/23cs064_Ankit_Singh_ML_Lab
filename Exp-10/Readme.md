A Markov Decision Process (MDP) is a mathematical framework used to model decision-making in environments where outcomes are partly random and partly under an agentâ€™s control. An MDP is defined by five components:

States (S)

Actions (A)

Transition probabilities (T)

Rewards (R)

Discount factor (Î³)

In this experiment, a 3Ã—4 GridWorld environment is modeled as an MDP. The agent can move in four directions, but movement is stochastic due to slipping probabilities. Certain cells act as terminal states (goal and pit), and one cell is a wall.

To solve the MDP, the Value Iteration algorithm is implemented from scratch. Value Iteration repeatedly applies the Bellman Optimality Equation to estimate the long-term value of every state:

ğ‘‰
(
ğ‘ 
)
=
max
â¡
ğ‘
âˆ‘
ğ‘ 
â€²
ğ‘‡
(
ğ‘ 
,
ğ‘
,
ğ‘ 
â€²
)
[
ğ‘…
(
ğ‘ 
â€²
)
+
ğ›¾
ğ‘‰
(
ğ‘ 
â€²
)
]
V(s)=
a
max
	â€‹

s
â€²
âˆ‘
	â€‹

T(s,a,s
â€²
)[R(s
â€²
)+Î³V(s
â€²
)]

The process continues until convergence. After computing the final value function, an optimal policy is extracted by selecting, for each state, the action that yields the highest expected value.

The results are visualized using heatmaps for the value function and arrow maps for the optimal policy. By changing the â€œliving penalty,â€ the experiment shows how reward shaping affects the agentâ€™s behavior and path selection.

This experiment demonstrates the fundamentals of planning in reinforcement learning and how optimal decision-making emerges from iterative evaluation of state values.
